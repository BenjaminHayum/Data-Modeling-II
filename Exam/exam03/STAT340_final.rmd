---
title: "STAT340 Final Exam, Fall 2022"
author: "Ben Hayum"
date: "12/19/2022"
output: html_document
---

## Instructions

Update the "author" and "date" fields in the header and complete the exercises below.
When you have completed the exam, knit the document, and submit **both the HTML and RMD** files to Canvas.

__Reminder:__ exams turned in more than three hours after starting the exam or after the close of the exam window at 12:05pm on Thursday, December 22nd  __will not__ be accepted.
If you wish to be careful, you may want to make a point to knit your document every so often while completing the exam.

You __may not__ discuss this exam with any person other than the instructor and the TAs until after the exam has closed (i.e., after the quiz is no longer accessible on Canvas).
You may use any and all course materials, including lecture notes, textbooks, other readings, previous homeworks, and the built-in R documentation.
You __may not__ use any resources not linked to by one or more of the canvas page, lecture notes or the course webpage.

Violation of these restrictions, such as discussing the exam with other students or searching for solutions online, will result in a zero on the exam and will be reported to the Office of Student Conduct. 

```{r}
# This chunk is to ensure that your exam knits with
# the "same" randomness each time. Do not delete it.
set.seed(340);
```

***

## Problem 1: the inverse trick

Consider the following cumulative distribution function:
$$
F(t) = \frac{ 1 }{ 1 + \exp\{ -t \} }
$$

### Part a) invert $F$

Do a bit of algebra to invert $F$.
That is, find the function $F^{-1} : [0,1] \rightarrow \mathbb{R}$ such that $F( F^{-1} (u) ) = u$ for all $u \in [0,1]$ and $F^{-1}( F(t) ) = t$ for all $t \in \mathbb{R}$.
__Hint:__ remember that $\log \exp\{ z \} = \exp\{ \log z \} = z$.

Implement this inverse function as `Finv`, which takes a single numerical input between `0` and `1` and returns a number.
You may assume that the input is numerical and lies strictly between 0 and 1, so there is no need to perform any error checking and no need to worry about infinities or division by zero.
```{r}
Finv <- function( u ) {
  return(  -log( (1/u) - 1 )  )
}

#Test:
1/(1 + exp(-Finv(0.1)))
```

### Part b) implement the inverse trick

Use the inverse trick to implement a function `generate_RV`, which takes a single positive integer argument `n` and returns a vector of `n` independent draws from the cumulative distribution $F$ given above.

```{r}
generate_RV <- function( n ) {
  inputs = runif(n = n, min = 0, max = 1)
  draws = rep(NA, n)
  for (i in 1:n) {
    draws[i] = Finv(inputs[i])
  }
  return(draws)
}
```

### Part c) implement the density

If you take the derivative of $F(t)$ with respect to $t$, you'll find that the density of our random variable is given by
$$
f(t) = \frac{d}{dt} F(t) = \frac{ e^{-t} }{ (1+e^{-t})^{2} }.
$$
Implment a function, `RVdensity` that takes a single numerical argument `t` and returns $f(t)$.
```{r}
RVdensity <- function( t ) {
  return(   exp(-t)/( (1 + exp(-t))^2 )   )
}
```

### Part d) sanity check with the density

Use your function from Part (b) to generate `n=10,000` samples from the CDF $F$.
Create a *normalized* histogram of those $10,000$ points, and use your function from Part (c) to plot the density, in red, over that histogram.
You are free to use either `ggplot2` or the built-in plotting tools in R.
__Hint:__
If all has gone well, the density and the histogram should look similar.
If they don't, that's a sign that you have a bug somewhere.

```{r}
library(ggplot2)

n = 10000
inputs = runif(n = n, min = 0, max = 1)
cdf_samples = generate_RV(n)

df = data.frame(inputs = inputs, cdf_samples = cdf_samples)
ggplot(data = df) + 
  geom_histogram(aes(x = cdf_samples, y = ..density..), binwidth = 0.075) + 
  stat_function(fun = RVdensity, aes(x = inputs), color = "red")
```

***

## Problem 2: two-sample testing

Famed pet products start-up Catamaran is redesigning their website.
They have two prospective redesigns, code-named Calico and Tabby.
A small fraction of visitors to Catamaran's are randomly shown either potential redesign Calico or potential redesign Tabby, and the amount in dollars that the customer spends is during their visit to the site is recorded.
The resulting data are given below.
```{r}
tabby <- c( 29.13, 99.18,  73.67, 115.76,  0.00,   0.00,  86.82,
            34.70,  0.00,   0.00,   0.00,  0.00,   0.00,   0.00,
            97.11,  0.00,  79.97,   0.00, 15.36, 102.44, 120.90,
            91.53, 67.94, 124.55, 111.31,  0.00,  81.87,   0.00,
            73.56, 38.10, 120.52,   0.00 );
calico <- c(  0.00,   0.00,   0.00,   0.00, 78.31,   0.00,  58.48,
             88.89,   0.00,  27.44,  91.21, 75.59,  87.70,  80.67,
             70.83,   0.00,   0.00,  43.71, 91.69,  45.74,  33.95,
             16.94, 107.08,   0.00,  81.30, 47.56,  27.00,   0.00 );
```

Let $\mu_T$ denote the population mean of dollars spent by a random user shown the Tabby redesign, and let $\mu_C$ denote the population mean of dollars spent by a random user shown the Calico redesign.
The marketing team at Catamaran wishes to assess which (if any) of the two potential redesigns is better, as measured by producing a higher mean dollar amount in customer spending.
That is, they wish to test the null hypothesis
$$
H_0 : \mu_T = \mu_C.
$$

### Part a) arrange an awesome test statistic

Choose a test statistic for assessing the null hypothesis given above and implement it as a function `catamaran_teststat`, which takes two vectors (corresponding to the two samples, in whichever order you prefer) and returns your chosen test statistic computed on the data.
*You should use this test statistic throughout the rest of this problem.*

```{r}
catamaran_teststat <- function(tabby, calico) {
  return( mean(tabby) - mean(calico) )
}
```

Why did you choose this test statistic?
What about this function makes it a good measure of how "unusual" a particular set of observations is?

***

I chose this test statistic of difference of means to test the null hypothesis that the means are equal because if it is the case that the means are not equal and are in fact different, then the difference of means should not be 0. The further the observed difference of samples means are from 0, the more unusual they are.

***

### Part b) produce a purrfect p-value with permutation testing

Use a permutation test to produce a p-value associated to the null hypothesis given above.
Your permutation test should use the test statistic that you chose in Part (a) at least $10,000$ Monte Carlo replicates.

```{r}
observed_test_stat = catamaran_teststat(tabby, calico)
observed_test_stat

permute_and_get_one_null_test_stat = function(tabby, calico) {
  pooled_data = c(tabby, calico)
  n_tabby = length(tabby)
  n_calico = length(calico)

  shuffled_data = sample(pooled_data, size=(n_tabby+n_calico), replace=FALSE)
  shuffled_tabby = shuffled_data[1:n_tabby]
  shuffled_calico = shuffled_data[(n_tabby+1):(n_tabby+n_calico)]
  
  return(catamaran_teststat(shuffled_tabby, shuffled_calico))
}

NMC = 10000
null_test_stats = rep(NA, NMC)
for (i in 1:NMC) {
  null_test_stats[i] = permute_and_get_one_null_test_stat(tabby, calico)
}

hist(null_test_stats)
abline(v=observed_test_stat, lwd=2, col='red')
abline(v=-observed_test_stat, lwd=2, col='red')


p_value = (sum(null_test_stats <= -abs(observed_test_stat)) + sum(null_test_stats >= abs(observed_test_stat)))/NMC
p_value
```

### Part c) concoct a Catamaran conclusion 

Suppose that the marketing team at Catamaran wish to test the null hypothesis above at level $\alpha=0.05$.
Based on your p-value from Part (b), should they accept the null hypothesis?

***

No. Using the permutation test, we got an approximated p-value of .4998 which is greater than $\alpha=0.05$ and is not statistically significant. Therefore, we do no have sufficient evidence to reject the null. We do not accept the null hypothesis, we just fail to reject it.

***

## Problem 3: Guinea Pigs

The `ToothGrowth` data set, which comes packaged in R, contains data on the lengths (the `len` column) of odonotoblasts (tooth growth cells) in 60 guinea pigs.
Each guinea pig in the study received doses of Vitamin C in amounts either 0.5, 1 or 2 mg per day (the `dose` column), either via orange juice or in the form of ascorbic acid (the `supp` column; a factor with `VC` encoding ascorbic acid and `OJ` encoding orange juice).

See `?ToothGrowth` for details.

### Part a) fit a linear model

Fit a linear model to the `ToothGrowth` data set that predicts `len` from an intercept term and the two other variables.
You *should not* include an interaction term.
Save the resulting model in a variable called `gpig_lm`.

__Note:__ the units of the `len` column are unspecified, to the best of my knowledge. You may simply refer to it as being measured in "units of length" or something similar.

__Note:__ the `supp` column is a factor, rather than being encoded as 0-1. Recall that R will handle this fact for you automatically, so there is no need to alter the data frame.

```{r}
gpig_lm <- lm(len ~ 1 + supp + dose, data = ToothGrowth)
```

### Part b) analyze and interpret a coefficient

Consider the model fitted in Part (a).

- Is the coefficient of the `dose` predictor statistically significantly different from zero at level $\alpha=0.001$?
- Give an interpretation of the estimated value.

```{r}
summary(gpig_lm)
```

***

The coefficient of the dose predictor has a p-value of 6.31e-16 which is statistically significant at $\alpha=0.001$

The interpretation of dose's coefficient in the linear model is that a unit increase in dose is associated with an increase of 9.7636 in tooth length

***

### Part c) include an interaction term

Fit a linear model to the `ToothGrowth` data set that predicts `len` from an intercept term, the two other variables and an interaction between `dose` and `supp`.
Save the resulting model in a variable called `gpig_inter_lm`.

```{r}
#TODO: replace NA below with something more appropriate
gpig_inter_lm <- lm(len ~ 1 + supp + dose + dose:supp, data = ToothGrowth)
```

### Part d) analyze and interpret the interaction term

Consider the model fitted in Part (c).

- Give a 95% confidence interval for the true value of the interaction coefficient.
- Give a brief exlanation of how to interpret the estimated value of this coefficient.

```{r}
confint(gpig_inter_lm, level = .95)
summary(gpig_inter_lm)
```

***

The 95% confidence interval for the true value of the interaction coefficient can be seen above

The interpretation of the interaction coefficient is that a unit increase in dose is  associated with an increase in tooth length that differs by 3.904 across the cases where the supplement is VC of OJ. 

In other words, depending on whether the supplement is VC or OJ, the change associated with a unit increase in dose -- aka the slope of dose vs length when controlling for supplement -- will change by 3.904.


***

### Part e) use CV to compare two models

Use $5$-fold cross-validation to compare the RSS of the linear model *without* an interaction term fitted in Part (a) against the RSS of the linear model *with* an interaction term fitted in Part (c).
Which is a better model, at least under this comparison?
Give a brief explanation as to your decision.

```{r}
K = 5

n <- nrow(ToothGrowth)
Kfolds <- split( sample(1:n, n ,replace=FALSE), as.factor(1:K));

residuals_linear = rep(NA, K)
residuals_interaction = rep(NA, K)

for (k in 1:K) {
  # For some reason requires double brackets
  curr_K = Kfolds[[k]]
  
  train_data <- ToothGrowth[-curr_K,]; # Everything besides the k-th fold
  leftout <- ToothGrowth[curr_K,]; # The k-th fold
  
  linear_model <- lm(len ~ 1 + supp + dose, data = ToothGrowth)
  linear_prediction <- predict( linear_model, leftout );
  residuals_linear[k] = sum((linear_prediction - leftout$len)^2)
  
  interaction_model = lm(len ~ 1 + supp + dose + dose:supp, data = ToothGrowth)
  interaction_prediction <- predict( interaction_model, leftout );
  residuals_interaction[k] = sum((interaction_prediction - leftout$len)^2)
}

mean(residuals_linear)
mean(residuals_interaction)
```

***

Under this comparison, the linear model with an interaction term had a lower mean RSS across the 5 K-folds than the linear model without an interaction term. Therefore, the model with the interaction term performed better.

***

## Problem 4: logistic regression

Consider the following (fictionalized) data frame,
in which each row encodes a subject in a study.
The column `X` encodes a measure of sugar in the diet,
and the `Y` column encodes whether or not a patient has been diagnosed with diabetes.

```{r}
x <- c( -0.64, -0.44, 1.48, 3.05, 1.94, 1.37, 1.40, 1.00, -0.22,
        0.73, -0.95, 0.44, -1.97, 1.36, -0.51, 3.05, 2.10, 0.37,
        -2.29, 1.86, 1.57, 1.40, -0.03, -0.18, -0.63, 1.78, 1.26,
        1.58, 1.74, 4.20 );
y <- c( 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1 );
diabetes_fictionalized <- data.frame(X=x, Y=y)
```

### Part a) logistic regression

Fit a logistic regression model to this data that predicts the probability of being diagnosed with diabetes (i.e., $\Pr[ Y=1 ]$) from the variable `X` (and an intercept term).

```{r}
#TODO: replace NA with something sensible.
diabetes_lr <- glm( Y ~ 1 + X, data=diabetes_fictionalized, family="binomial");
```

### Part b) interpreting a coefficient

Answer the following:
- What is your model's point estimate for the coefficient of the `X` variable?
- Give a brief explanation of the interpretation of this coefficient.
- Give a 95% confidence interval for the true value of this coefficient.

```{r}
summary(diabetes_lr)
confint(diabetes_lr, level = .95)
```

***

The model's point estimate for the coefficient of the `X` variable is 2.0760

The interpretation of this coefficient is that 2.0760 is the increase in the log-odds of the individual having diabetes that associated with a unit increase in `X`, a measure of sugar in the diet.

A 95% confidence interval for the true value of this coefficient is shown above.

***

## Problem 5: Confidence intervals three ways

Consider the following data, generated according to an exponential distribution with unknown rate parameter $\lambda_0$.

```{r}
p7_data <- c( 4.65, 4.02, 0.40, 0.34, 4.42, 0.32, 2.80, 1.76, 1.22,
              5.97, 6.66, 4.09, 0.61, 1.94, 4.70, 10.9, 0.25, 5.09,
              0.14, 2.49, 0.69, 3.09, 1.04, 3.75, 0.27, 4.83, 4.78,
              0.071, 0.28, 1.04 );
```

__Reminder:__ the expectation of an exponential random variable with rate $\lambda$ is $1/\lambda$, so a natural choice of estimate for $\lambda_0$ is the reciprocal of the sample mean, $1/\bar{X}$.

### Part a) Simulation-based CI

Using the estimate $1/\bar{X}$, construct a simulation-based 95% confidence interval for the rate parameter $\lambda_0$.

```{r}
lambda_hat = 1/mean(p7_data)

n = length(p7_data)
NMC = 10000
lambda_star_array = rep(NA, NMC)
for (i in 1:NMC) {
  data_star = rexp(n = n, rate = lambda_hat)
  lambda_star_array[i] = 1/mean(data_star)
}

hist(lambda_star_array)

alpha = .05
lowerBound = alpha/2
upperBound = 1 - (alpha/2)

quantile(lambda_star_array, c(lowerBound, upperBound))
```

### Part b) CLT-based CI

Construct a CLT-based 95% confidence interval for the rate parameter $\lambda_0$.

```{r}
lambda_hat = 1/mean(p7_data)
n = length(p7_data)

# Not sure how to get the right variance here...
# The variance of lambda_hat is not going to be the variance of the data
#     lambda_hat is 1/mean(data) so maybe do 1/var(data)??
var2hat = 1/var(p7_data)

alpha = .05
zstar = abs(qnorm(alpha/2))

lowerBound = lambda_hat - zstar*sqrt(var2hat/n)
upperBound = lambda_hat + zstar*sqrt(var2hat/n)

CI = c(lowerBound, upperBound)
CI

# zstar*sqrt(var2hat/n)
# should be around .135 to match the simulation and bootstrap's margins of error
```

### Part c) bootstrap CI

Use the bootstrap to construct a 95% confidence interval for the rate parameter $\lambda_0$.
Your implementation should use at least $200$ bootstrap replicates.

```{r}
lambda_hat = 1/mean(p7_data)
n = length(p7_data)

B <- 10000; # Number of bootstrap replicates.
replicates <- rep(NA,B); # We'll store
for( i in 1:B ) {
  # Sample WITH REPLACEMENT from the data sample itself.
  resample <- sample(p7_data, n, replace=TRUE);
  replicates[i] <- 1/mean(resample);
}

hist(replicates)

alpha = .05
lowerBound = alpha/2
upperBound = 1 - (alpha/2)

quantile(replicates, c(lowerBound, upperBound))
```

## Problem 6: Monte Carlo estimation

Consider two normal random variables, $X$ and $Y$, with

- $X$ generated according to a normal with mean $\mu_X=1$ and variance $\sigma^2_X =3$, and 
- $Y$ generated according to a Poisson with rate parameter $\lambda_Y = 2$.

Suppose we are interested in the probability that $Y > X$.
That is, we want to estimate
$$
p_{Y > X} = \Pr[ Y > X]
$$

### Part a) Monte Carlo estimation for $p_{Y>X}$

Use Monte Carlo to construct an estimate of $p_{Y > X}$.
Your simulation should use at least $10^4$ Monte Carlo replicates.

```{r}
NMC = 10^4

Y_greater_than_X = rep(NA, NMC)
for (i in 1:NMC) {
  X = rnorm(n = 1, mean = 1, sd = sqrt(3))
  Y = rpois(n = 1, lambda = 2)
  if (Y > X) {
    Y_greater_than_X[i] = 1
  } else {
    Y_greater_than_X[i] = 0
  }
}

mean(Y_greater_than_X)
```

### Part b) confidence interval of $p_{Y>X}$

Construct a CLT-based 95% confidence interval for $p_{Y>X}$.
__Hint:__ your estimate stored in Part (a) is the mean of a collection of Bernoulli random variables with shared mean $p_{Y>X}$.

```{r}
p_hat = mean(Y_greater_than_X)
n = length(Y_greater_than_X)
# Not sure what the variance is is the variance of the p_hat distribution is just equal to the variance of the Bernouli distribution divided by n? Not sure what else it could be

var2hat = var(Y_greater_than_X)

alpha = .05
zstar = abs(qnorm(alpha/2))

lowerBound = p_hat - zstar*sqrt(var2hat/n)
upperBound = p_hat + zstar*sqrt(var2hat/n)

CI = c(lowerBound, upperBound)
CI
```





## EXTRA: Ben's tests for the multiple choice quesitons
### Question 7
```{r}
simple_linear = lm(mpg ~ 1 + disp, data = mtcars)
predict(simple_linear, data.frame(disp = mean(mtcars$disp)))
mean(mtcars$mpg)

simple_linear = lm(len ~ 1 + dose, data = ToothGrowth)
predict(simple_linear, data.frame(dose = mean(ToothGrowth$dose)))
mean(ToothGrowth$len)
```
They are the same in both cases so the answer is true


### Question 8
```{r}
X = rnorm(n = 200, mean = 1, sd = sqrt(2))
Y = rnorm(n = 200, mean = 0, sd = sqrt(1/2))
simple_linear = lm(Y ~ 1 + X)
summary(simple_linear)
```
Ran this a bunch of times and the coefficient of X was always somewhere around 0 --> pick the number closest to 0

### Question 9
#### LOOKING AT TRAIN RSS
```{r}
library(glmnet)
lambda_vals = seq(from=0, to=15, by = (.01))
n <- nrow(mtcars)
y_mtc <- mtcars[,1]
x_mtc <- mtcars[, -c(1)]

iteration = rep(NA, length(lambda_vals))
residuals = rep(NA, length(lambda_vals))
for (i in 1:length(lambda_vals)) {
  mtc_lasso_lambda <- glmnet(as.matrix(x_mtc), y_mtc, alpha = 1, lambda=lambda_vals[i]);
  prediction <- predict(mtc_lasso_lambda, as.matrix(x_mtc))
  iteration[i] = i
  residuals[i] = sum((prediction - y_mtc)^2)

}

plot(iteration, residuals)
```


### Question 10
#### LOOKING AT TEST RSS
```{r, warning = FALSE}
library(glmnet)
lambda_vals = seq(from=0, to=15, by = (0.01))
K = 5
n <- nrow(mtcars)

iteration = rep(NA, length(lambda_vals))
mean_residuals = rep(NA, length(lambda_vals))
for (i in 1:length(lambda_vals)) {
  residuals = rep(NA, K)
  Kfolds = split( sample(1:n, n ,replace=FALSE), as.factor(1:K) )
  
  for (k in 1:K) {
    # For some reason requires double brackets
    curr_K = Kfolds[[k]]
    
    train_data <- mtcars[-curr_K,]; # Everything besides the k-th fold
    y_train = train_data[, 1]
    x_train = train_data[, -c(1)]
    
    leftout <- mtcars[curr_K,]; # The k-th fold
    y_leftout = leftout[, 1]
    x_leftout = leftout[, -c(1)]
    
    mtc_lasso_lambda <- glmnet(as.matrix(x_train), y_train, alpha = 1, lambda=lambda_vals[i]);
    prediction <- predict(mtc_lasso_lambda, as.matrix(x_leftout))
    residuals[k] = sum((prediction - y_leftout)^2)
  }
  mean_residuals[i] = mean(residuals)
  iteration[i] = i
}

plot(iteration, mean_residuals)
```

