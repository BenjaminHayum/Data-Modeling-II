---
title: "STAT340 Exam 2, Fall 2022"
author: "Ben Hayum"
date: "Nov 14, 2022"
output: html_document
---

## Instructions

Update the "author" and "date" fields in the header and complete the exercises below.
When you have completed the exam, knit the document, and submit **both the HTML and RMD** files to Canvas.

__Reminder:__ exams turned in more than three hours after starting the exam or after the close of the exam window at 3pm on Wednesday November 16th __will not__ be accepted.
If you wish to be careful, you may want to make a point to knit your document every so often while completing the exam.

You __may not__ discuss this exam with any person other than the instructor and the TAs until after the exam has closed (i.e., after the quiz is no longer accessible on Canvas).
You may use any and all course materials, including lecture notes, textbooks, other readings, previous homeworks, and the built-in R documentation.
You __may not__ use any resources not linked to by either the canvas page, lecture notes or the course webpage.

Violation of these restrictions, such as discussing the exam with other students or searching for solutions online, will result in a zero on the exam and will be reported to the Office of Student Conduct. 

***

__IMPORTANT:__ Do not delete this block of code. It sets the seed on the random number generator so that your random experiments will have the same outputs every time you knit the document.
```{r}
set.seed(1848)
```

## Problem 1: CIs for the Mean of an Exponential

The following data were drawn independently according to an exponential distribution with rate parameter $\lambda$.
Our goal is to estimate the *mean*, $\mathbb{E} X_1 = 1/\lambda$.
__Note:__ be mindful throughout this problem of the fact that the mean of the exponential is the reciprocal of the rate parameter!

```{r}
expo_data<- c( 6.98, 7.72, 1.92, 0.51, 1.31, 7.75, 0.34, 5.15, 4.18, 8.50,
               1.70, 2.61, 0.66, 2.01, 2.72, 1.44, 9.27, 6.16, 0.07, 11.44,
               0.85, 9.89, 8.94, 1.64, 3.12, 5.51, 2.07, 0.618, 1.33, 4.11,
               0.04, 0.36, 1.85, 3.50, 4.25 );
hist(expo_data)
```

### Part a) simulation-based CI for the mean

Use the data to construct a *simulation-based* two-sided $95\%$ confidence interval *for the mean* $\mathbb{E} X_1 = 1/\lambda$.
Your simulation should use at least $10,000$ Monte Carlo iterates.

```{r}
xbar = mean(expo_data)

lambda_hat = 1/mean(expo_data)
n = length(expo_data)
NMC = 10000
xbar_star_array = rep(NA, NMC)
for (i in 1:NMC) {
  data_star = rexp(n = n, rate = lambda_hat)
  xbar_star_array[i] = mean(data_star)
}

hist(xbar_star_array)

alpha = .05
lowerBound = alpha/2
upperBound = 1 - (alpha/2)

unname(quantile(xbar_star_array, c(lowerBound, upperBound)))
```

### Part b) CLT-based CI for the mean

Use the data to construct a two-sided $95\%$ confidence interval *for the mean* $\mathbb{E} X_1 = 1/\lambda$ using the central limit theorem (CLT) approximation.

```{r}
xbar = mean(expo_data)
n = length(expo_data)
var2hat = var(expo_data)

alpha = .05
zstar = abs(qnorm(alpha/2))

lowerBound = xbar - zstar*sqrt(var2hat/n)
upperBound = xbar + zstar*sqrt(var2hat/n)

CI = c(lowerBound, upperBound)
CI
```

### Part c) simulation-based CI for the rate parameter

Use the data to construct a *simulation-based* two-sided $95\%$ confidence interval *for the rate parameter* $\lambda = 1/\mathbb{E} X_1$.
Your simulation should use at least $10,000$ Monte Carlo iterates.

```{r}
lambda_hat = 1/mean(expo_data)
n = length(expo_data)
NMC = 10000
lambda_star_array = rep(NA, NMC)
for (i in 1:NMC) {
  data_star = rexp(n = n, rate = lambda_hat)
  lambda_star_array[i] = 1/mean(data_star)
}

hist(lambda_star_array)

alpha = .05
lowerBound = alpha/2
upperBound = 1 - (alpha/2)

unname(quantile(lambda_star_array, c(lowerBound, upperBound)))
```

### Part d) CLT-based CI for the rate parameter

Use the data to construct a two-sided $95\%$ confidence interval *for the rate parameter* $\lambda = 1/\mathbb{E} X_1$ using the central limit theorem (CLT) approximation.

```{r}
lambda_hat = 1/mean(expo_data)
n = length(expo_data)

# Unsure of how to get the right variance of lambda and not of X which is what we need to do a CLT method...
# Variance(lambda) = E(lambda^2) - E(lambda)^2
# Variance(lambda) = E((1/E(X))^2) - E(1/E(X))^2    ????

# I don't think we're able to do this without knowing the variance of lambda and there's no way to calculate it here...

#var2hat = (1/mean(expo_data))^2 - (1/mean(expo_data))^2
var2hat = 1/var(expo_data)
#var2hat = var(expo_data) makes a confidence interval that's wayyy too large

alpha = .05
zstar = abs(qnorm(alpha/2))

lowerBound = lambda_hat - zstar*sqrt(var2hat/n)
upperBound = lambda_hat + zstar*sqrt(var2hat/n)

CI = c(lowerBound, upperBound)
CI

```

## Problem 2: A/B Testing

World-renowned pet supply company Catamaran is preparing to launch its new social media platform for cats.
The goal of the designers is to maximize user engagement, as measured by time spent on the site (measured in minutes).
The designers of the Catamaran user interface have narrowed things down to two possible site designs, A and B.
The designers have randomly assigned users to design A or B, and measured time in the app for each design (rounded to the nearest number of minutes).
The resulting data is below.

```{r}
minutesA <- c( 0,5,4,5,1,0,1,1,2,2,0,11,3,0,4,24,3,1,3,1,6,2,1,4,0,8,6,3);
minutesB <- c( 7,1,0,2,2,0,3,2,5,0,0,0,0,2,0,1,1,5,3,2,1,1,1,0,1,0,1,4,0,0,4,5);
```

Let $\mu_A$ and $\mu_B$ denote the (true but unknown) means of the times spent by the users assigned to design A and B, respectively.

### Part a) Constructing point estimates for the means

Use the plug-in principle to obtain point estimates for $\mu_A$ and $\mu_B$.

```{r}
xbar_A = mean(minutesA)
xbar_A
xbar_B = mean(minutesB)
xbar_B
```

### Part b) Constructing a point estimate for the treatment effect

Construct a point estimate of the *treatment effect*
$$
\delta = \mu_A - \mu_B.
$$

Is the "effect" of switching from design B to design A positive or negative?
That is, is your estimate of $\delta$ positive or negative?

__Hint:__ Note that $\mu_A = \mathbb{E} X$, where $X$ is the time spent by a random user in design $A$, and use the plug-in principle.

```{r}
delta = xbar_A - xbar_B
delta
# The effect is positive
```

### Part c) Estimating model parameters

Suppose that we model time spent by users in treatment A as being drawn from a geometric random variable with success probability $p_A$, and users in treatment B as being drawn from a geometric random variable with success probability $p_B$.
In R's notation, a geometric random variable $X$ with success probability $p$ has expected value
$$
\mathbb{E} X = \frac{1-p}{p}.
$$

Solving for $p$, we have
$$
p = \frac{1}{1 + \mathbb{E} X }.
$$

Use the plug-in principle to obtain point estimates for $p_A$ and $p_B$ in terms of your estimates of $\mu_A$ and $\mu_B$ from Part (a).

```{r}
phat_A = 1/(1 + xbar_A)
phat_A
phat_B = 1/(1 + xbar_B)
phat_B

```

### Part d) Simulation-based CI for $\delta$

Continuing to assume that the data were drawn from geometric distributions, use your estimates of $p_A$ and $p_B$ from Part (c) to obtain a simulation-based two-sided $95\%$ confidence interval for the effect size $\delta$ defined above.
Your simulation should use at least 10,000 Monte Carlo replicates.

__Hint:__ if you've estimated $p_A$, then you can generate "fake" data from design A, and similar for design B using your estimate of $p_B$.
Having generated "fake" data for each design condition, just construct the same plug-in estimate as you used for $\delta$ in Part (b) above, but using the "fake" data instead of the original `minutesA` and `minutesB`.

```{r}
#phat_A calculated above
#phat_B calculated above
n_A = length(minutesA)
n_B = length(minutesB)
NMC = 10000
delta_star_array = rep(NA, NMC)
for (i in 1:NMC) {
  fakedata_A = rgeom(n = n_A, prob = phat_A)
  fakedata_B = rgeom(n = n_B, prob = phat_B)
  delta_star_array[i] = mean(fakedata_A) - mean(fakedata_B)
}

hist(delta_star_array)

alpha = .05
lowerBound = alpha/2
upperBound = 1 - (alpha/2)

unname(quantile(delta_star_array, c(lowerBound, upperBound)))

```

### Part e) Permutation testing

Use a two-sided permutation test with test statistic given by the difference of means to test the null hypothesis
$$
H_0 : \mu_A = \mu_B,
$$
or, equivalently, the hypothesis
$$
H_0 : \delta = 0.
$$

Conduct your test at the $\alpha=0.05$ level using at least $10,000$ Monte Carlo replicates.

```{r}
treatment = minutesA
control = minutesB

test_stat = function(treatment, control) {
  return(mean(treatment) - mean(control))
}

observed_test_stat = test_stat(treatment, control)
observed_test_stat

permute_and_get_one_null_test_stat = function(treatment, control) {
  pooled_data = c(treatment, control)
  n_control = length(control)
  n_treatment = length(treatment)

  shuffled_data = sample(pooled_data, size=(n_control+n_treatment), replace=FALSE)
  shuffled_control = shuffled_data[1:n_control]
  shuffled_treatment = shuffled_data[(n_control+1):(n_control+n_treatment)]
  
  return(test_stat(shuffled_treatment, shuffled_control))
}

NMC = 10000
null_test_stats = rep(NA, NMC)
for (i in 1:NMC) {
  null_test_stats[i] = permute_and_get_one_null_test_stat(treatment, control)
}

hist(null_test_stats)

p_value = (sum(null_test_stats <= -abs(observed_test_stat)) + sum(null_test_stats >= abs(observed_test_stat)))/NMC
p_value
```

***

The approximated p-value of our two-sided permutation test is about .024. This is less than our significance level of .05 and therefore provides sufficient evidence to reject the null hypothesis that mu_A is equal to mu_B/that delta is equal to 0.

***

Does your conclusion match the one that would be obtained if you inverted your confidence intervals in Part (d)?
Does this surprise you?
Why or why not?

***

Yes. The confidence interval for delta from Part (d) did not include 0 and only had values above it. Therefore, it gave sufficient evidence to reject the null hypothesis that mu_A = mu_B/that delta = 0. The same was found here with a 2 sided permutation test that also returned statistical significance against the shared null hypothesis.

***

## Problem 3: Estimating Conditional Probabilities

At a recent conference, every attendee at the conference banquet was asked to specify whether they wanted a meal with meat or a vegan meal.
When it came time to order wine, I noticed that most of the people who had ordered meat asked for red wine, while most people who had ordered the vegan option asked for white.
Some people from both groups asked for neither and had water.
Below is my best recollection of how the 200 or so conference attendees ordered (okay, okay, this data is totally made up).
The columns correspond to whether an attendee ordered the meat meal or the vegan meal.
The rows correspond to whether the attendee ordered red or white wine.

$$
\begin{aligned}
& ~~~~ {\text{ Meat }} & {\text{ Vegan } } & ~~~~~~ { \bf \text{ Total } } \\
{ \text{ Red } } & ~~~~~~ \text{ 82 } & \text{ 26 } ~~~ & ~~~~~~ \text{ 108 } \\
{ \text{ White } } & ~~~~~~ \text{ 31 } & \text{ 61 } ~~~ & ~~~~~~ \text{ 92 } \\
{ \text{ Water} } & ~~~~~~ \text{11} & \text{ 14 } ~~~ & ~~~~~~ \text{ 25 } \\
{ \bf \text{Total} } & ~~~~~~ \text{ 124 } & \text{ 101 } ~~~ & ~~~~~~ {\bf \text{ 225 }}
\end{aligned}
$$
### Part a) Estimating a marginal probability

From the data above, estimate the probability that a random conference attendee orders a vegan meal with red wine?

```{r}
26/225
```

### Part b) Estimating a conditional probability

From the data above, estimate the probability that a random conference attendee orders the meat meal, conditional on the event that they ordered white wine.

```{r}
31/92
```

### Part c) Estimating another conditional probability

From the data above, estimate the probability that a random conference attendee orders wine (either red or white), conditional on the fact that they ordered a vegan meal.

```{r}
(26+61)/101
```

### Part d) Disease screening

Suppose we are concerned with screening for a disease that occurs with a prevalence of $1$ in $100$ people (i.e., the probability that a randomly-chosen person has the disease is $1/100$).
Our screening test has a sensitivity of $95\%$ and a specificity of $99.9\%$ (refer to the lecture notes for definitions of these terms).
Suppose that a randomly-chosen person tests positive on our screening test.
What is the probability that this person has the disease, conditional on the event of this positive test?

```{r}
p_D = .01
p_notD = .99
p_pos_given_D = .95
p_neg_given_D = .05
p_pos_given_notD = .001
p_neg_given_notD = .999

# The probability of D given pos is:
(p_pos_given_D*p_D)/(p_pos_given_D*p_D + p_pos_given_notD*p_notD)
```

## Problem 4: Constructing a CI for the correlation coefficient

Suppose we have a pair of random variables $X$ and $Y$ that are jointly distributed as a *multivariate normal* with mean $\mu = (\mu_X, \mu_y)$ and covariance matrix
$$
\Sigma = \begin{bmatrix} 1 & \rho \\
                      \rho & 1 \end{bmatrix}
$$

Recall that this means that $X$ and $Y$ are both *marginally* normally distributed, with
$$
X \sim \operatorname{N}( \mu_X, 1 )~~~\text{ and }~~~
Y \sim \operatorname{N}( \mu_Y, 1 ),
$$

and $\operatorname{Cov}(X,Y) = \rho$.
Note that since $X$ and $Y$ both have variance $1$, $\rho$ is also the correlation between $X$ and $Y$.

```{r}
x <- c( -0.54660789, 2.43686553, 1.24397260, -0.73957557, 2.33131378,
        -0.97341652, -0.48911950, 0.35658630, -0.77824579, 1.55796356,
        0.10641176,  0.89764371, 0.64903636, 1.89822592, -0.35568742,
        1.52124163, 0.01328800, 1.33453143, -1.08829020, 0.83033763,
        0.68018922, 1.93419047, 1.14073253, 2.56650893, 1.90859128,
        1.39105347, 1.37547130, -0.07223141,  0.43142589, 1.78035482,
        1.90837367, 1.84173396, -0.09760825, 1.66610817, 0.03379679,
        0.97277154, 0.78599263,  0.75538460, 1.23006396, 0.83001685 );
y <- c( 0.64297443, 2.47047888, 2.20400164, 1.63743023, 2.71562759,
        0.89221406, 0.95695500, 1.93372675, -0.78510178, 1.67736371,
        0.84847015, 1.54191399, 1.06871388, -0.68989956, -0.08968257,
        1.03751238, 0.70287354, 0.18421545, -0.12443295, -1.46861684,
        0.48048808, 1.84445844, 0.19248018, 1.03515580, 1.13516935,
        1.54098541, 2.24399689, -1.12419549, 0.40755826, 1.03454528,
        -0.43858469, 0.82394013, -0.64106964, 0.90682616, 2.14253534,
        -0.16303192, 0.01571576, 0.05148464, 0.21077641, 0.54581579 )
```


### Part a) estimating $\rho$

Using the data above, construct a point estimate for the parameter $\rho$ described above.
You should assume that both $X$ and $Y$ have variance $1$.

```{r}
rho_hat = cov(x, y)
rho_hat
```

### Part b) hypothesis testing for $\rho$

Use a permutation test to associate a p-value to the null hypothesis
$$
H_0 : \rho = 0.
$$

Your permutation test should use at least 10,000 Monte Carlo replicates.

__Hint:__ under the null hypothesis, $X$ and $Y$ are independent, and so any permutation of the vector of $Y$ values is equally likely.

```{r}
test_stat = function(x, y) {
  return(cov(x, y))
}

observed_test_stat = test_stat(x, y)
observed_test_stat

permute_and_get_one_null_test_stat = function(x, y) {
  shuffled_x = sample(x, size=length(x), replace=FALSE)
  shuffled_y = sample(y, size=length(y), replace=FALSE)

  return(test_stat(shuffled_x, shuffled_y))
}

NMC = 10000
null_test_stats = rep(NA, NMC)
for (i in 1:NMC) {
  null_test_stats[i] = permute_and_get_one_null_test_stat(x, y)
}

hist(null_test_stats)

p_value = (sum(null_test_stats <= -abs(observed_test_stat)) + sum(null_test_stats >= abs(observed_test_stat)))/NMC
p_value
```

Use this p-value to test the null hypothesis above at the $\alpha=0.1$ level and then at the $\alpha=0.01$ level.

***

The two sided permutation test shown above returned an approximated p-value of .0443. If we set our significance level at $\alpha=0.1$, then p < alpha and we have sufficient evidence to reject the null hypothesis that $\rho=0$. If we set our significance level at $\alpha=0.01$, then p > alpha and we fail to reject the null hypothesis.

***

## Problem 5: CIs for the Second Moment

For a random variable $X$, the *second moment* of $X$ is defined to be $\mathbb{E} X^2$.
Rearranging the formula for the variance,
$$
\operatorname{Var} X = \mathbb{E} X^2 - \mathbb{E}^2 X,
$$

we can write the second moment of $X$ as
$$
\mathbb{E} X^2 = \sigma^2 + \mu^2,
$$

where $\mu$ is the mean of $X$ and $\sigma^2$ is the variance of $X$.

In this problem, we will work with the `morley` data set, which is built into R.
This data is from Michelson's (of the famed Michelson and Morley experiment) early attempts to measure the speed of light.
The data set has three columns, two of which correspond to experiment runs, and the third, labeled `Speed` corresponds to measurements of the speed of light in kilometers per second (but with the approximate true speed of light 299,000 km/s subtracted, which you can ignore).

```{r}
head(morley)
```

Below, you should ignore the `Expt` and `Run` columns, and assume that the entries of the `Speed` column are generated independently.

__Note:__ this is the hardest problem on the exam.
It is meant to challenge you and make you draw connections between a few different ideas from lecture.
Read the questions carefully and take your time.
When in doubt, add comments (either in the code or in clearly-marked plain text) explaining your reasoning.
A wrong answer that is at least well-motivated and/or clearly explained will get more credit than a wrong answer with no explanation.

### Part a) constructing a point estimate for the second moment

Construct a point estimate for the second moment of the `Speed` variable in the `morley` data set.

__Hint:__ use the plug-in principle discussed in lecture.

```{r}
var2hat = var(morley$Speed)
xbar = mean(morley$Speed)

secondmoment_hat = var2hat + xbar^2
secondmoment_hat
```

### Part b) Constructing a simulation-based CI for the second moment

Let's suppose that the speed of light data are drawn independently from a normal distribution with mean $\mu$ and variance $\sigma^2$.
Construct a two-sided $95\%$ simulation-based confidence interval for the second moment of the speed of light variable.
You should use at least 10,000 Monte Carlo replicates in your simulation.

__Hint:__ once we have estimated the mean and variance of a normal, we can generate new random variables from it (i.e., our "fake" data), and compute an estimate of the second moment from that sample, just as in Part (a) above.

```{r}
var2hat = var(morley$Speed)
xbar = mean(morley$Speed)
n = length(morley$Speed)

secondmoment_hat = var2hat + xbar^2

NMC = 10000
secondmoney_star_array = rep(NA, NMC)
for (i in 1:NMC) {
  fakedata = rnorm(n = n, mean = xbar, sd = sqrt(var2hat))
  secondmoney_star_array[i] = var(fakedata) + mean(fakedata)^2
}

hist(secondmoney_star_array)

alpha = .05
lowerBound = alpha/2
upperBound = 1 - (alpha/2)

unname(quantile(secondmoney_star_array, c(lowerBound, upperBound)))
```

### Part c) Constructing a CLT-based CI for the second moment

Construct a two-sided $95\%$ CLT-based confidence interval for the second moment of the speed of light variable.

__Hint:__ Don't get tripped up by our assumption in Part (b) that the speed of light data are normal. Instead, think about this: Suppose that we have a random variable $X$ and we define a new random variable $Y = X^2$.
The distribution of $X^2$ is the same as the distribution of $Y$.
So to construct a confidence interval for $\mathbb{E} Y$, we can just construct a confidence interval for $\mathbb{E} X^2$.
In other words, try working with `morley$Speed^2` directly.

```{r}
secondmoment_hat = var(morley$Speed) + mean(morley$Speed)^2
n = length(morley$Speed)

# Not sure if this is exactly the variance hat squared of the second moment because second moment depends on mean squared (in addition to the variance)

# But...if we are trying to construct an interval proportional to Speed^2 (in addition to the variance), it seems like it makes sense to find the variance of Speed^2...and the numbers turn out looking okay
samplevar = var(morley$Speed^2)

mean(morley$Speed^4) - mean(morley$Speed^2)

alpha = .05
zstar = abs(qnorm(alpha/2))

lowerBound = secondmoment_hat - zstar*sqrt(samplevar/n)
upperBound = secondmoment_hat + zstar*sqrt(samplevar/n)

CI = c(lowerBound, upperBound)
CI
```
