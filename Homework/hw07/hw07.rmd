
---
title: "STAT340 HW07: Estimation II"
author: "Ben Hayum"
date: "October 29, 2022"
output: html_document
---

***

No other students

***

## Instructions

Update the "author" and "date" fields in the header and
complete the exercises below.
Knit the document, and submit **both the HTML and RMD** files to Canvas.

__Due date:__ November 3, 2022 at 11:59pm.

---

This homework will review our discussion of estimation and confidence intervals from this week's lectures.

## 1) Comparing CLT- and simulation-based CIs

In lecture, we saw two different approaches to building confidence intervals.

* One, hopefully familiar to you from previous introductory courses, was based on the CLT, namely the fact that the sample mean is (approximately) normal about the population mean after appropriate rescaling (i.e., dividing by the standard deviation).
* The other, probably new to you, was simulation-based.
We estimate our parameter(s) of interest, then generate new data based on those (estimated) parameters.
We can then use that data to estimate the parameter(s) *again*, and the behavior of those "fake data" estimates can tell us about the behavior of our estimator with respect to the true parameter(s).

One thing we didn't discuss in lecture is how these two methods compare (except to say that... it depends).

This problem is fairly open-ended. Your job is to set up an experiment comparing these two different confidence interval methods.

### Part a: The data generating model

First things first, we need to generate data.
Define a function `generate_data`, that takes two arguments, `lambda` and `nsamp`, where `lambda` is the parameter of a Poisson and `nsamp` is a positive integer specifying a number of samples.
Your function should return a vector of length `nsamp`, whose entries are drawn iid from Poisson with parameter given by `lambda`.
`lambda` should default to 1.

```{r}
generate_data <- function( nsamp, lambda=1 ) {
  return( rpois(nsamp, lambda = lambda) )
}

```

### Part b: fitting the data

Our simulation-based method requires that we have an estimate of the mean, and our CLT-based method requires that we have estimates of the mean and variance.
Write functions `estimate_pois_mean` and `estimate_pois_var` to estimate both the mean and variance of the Poisson given the output of `generate_data`.

__Hint:__ the Poisson has the convenient property that the expectation and variance are both equal to $\lambda$, so these can be the same function-- both can just return the sample mean, if you want, but you can also use the sample variance. Your choice!

```{r}
estimate_pois_mean <- function( data ) {
  return( mean(data) )
}

estimate_pois_var <- function( data ) {
  # TODO: code goes here. Estimate the variance.

  # One option is to be lazy and use the fact that mean=var=lambda in Poisson
  # Like this: return( estimate_pois_mean( data ) );
  return( var(data) )
}

```

### Part c: CLT-based confidence intervals

Implement a function `CLT_CI` that takes arguments `data` (the vector of data generated by `generate_data`) and `alpha` (a numeric between 0 and 1, i.e, a probability, which should default to $0.05$), and returns a confidence interval (i.e., a vector of length 2 whose first entry is the left end of the interval and whose second entry is the right end of the interval).

This confidence interval should be based on our CLT-based approximation. That is, you should use the fact that
$$
  \frac{ \bar{X} - \lambda }{ \sqrt{ (\operatorname{Var X_1})/n }}
$$
is approximately normal.

Use your `estimate_pois_var` function to estimate the variance term in the denominator (ignore the fact that this is equal to $\lambda$), use `estimate_pois_mean` to get $\bar{X}$ and follow the basic outline from lecture to create a $(1-\alpha)*100$-percent *two-sided* confidence interval (if "two-sided" is not familiar to you, no worries-- just follow the recipe from lecture and you'll be fine!).

```{r}
CLT_CI <- function( data, alpha=0.05) {
  # TODO: code here.
  
  # Step 1. use data to get Xbar and the variance estimate.
  x_bar = estimate_pois_mean(data)
  var_x = estimate_pois_var(data)
  n = length(data)
    # Step 2. Use the estimated variance and choice of alpha to construct
  # a two-sided confidence interval for lambda
  # (sorry, Z-scores are going to show up here, but only briefly!)
  margin_of_error = abs( qnorm(alpha/2)*sqrt(var_x/n) )
  
  CI = c(x_bar - margin_of_error, x_bar + margin_of_error)
  # Return the CI. Be careful-- depending on how you got your CI, it
  # might not be a simple vector (e.g., it might have a header).
  # Feel free to use the lecture code for reference.
  return(CI)
}

```

### Part d: simulation-based confidence intervals

Implement a function `simulation_CI` that takes arguments `data` (the vector of data generated by `generate_data`) and `alpha` (a numeric between o and 1, i.e, a probability, which should default to $0.05$), and returns a confidence interval (i.e., a vector of length 2 whose first entry is the left end of the interval and whose second entry is the right end of the interval).

Your function should construct the confidence interval according to the simulation-based approach discussed in class.
You are free to adapt the code from lecture in your code.
You may pick the number of Monte Carlo iterates (i.e., replicates) as you wish, but we would recommend setting it to be at least a few hundred.

```{r}
simulation_CI <- function( data, alpha=0.05) {
  # TODO: code goes here.
  NMC = 1000
  n = length(data)/2
  
  lambda_hat = mean(data)
  lambda_star = rep(NA, NMC)
  for (j in 1:NMC) {
    data_star = rpois(n = n, lambda = lambda_hat)
    lambda_star[j] = mean(data_star)
  }
  
  lowerBound = alpha/2
  upperBound = 1-(alpha/2)
  return( quantile(lambda_star, probs = c(lowerBound, upperBound)) )
  
  # Return a vector c(L, U) with L the left end and U the right ("upper") end of the CI. 
}


```

### Part e: Comparing Confidence Intervals

So, now you've implemented two different confidence intervals.
Let's compare them.

First of all, we need to be able to check if our CI "caught" the true parameter or not.

Write a function `contained` that takes a CI (i.e., a two-vector with first entry smaller than the second) and a number (the true value of the parameter) that returns TRUE if the true value of the parameter is inside the interval and FALSE otherwise.

```{r}
contained <- function( myCI, trueparam ) {
  # TODO: code goes here.
  # Hint: the estimation lecture notes include code to do this.
  if (trueparam > myCI[1] && trueparam < myCI[2]) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}

```

What does it mean for one confidence interval to be better than another?
We define the *coverage* of a confidence interval to be the probability that the true parameter lies in the interval.

Ideally, a $1-\alpha$ confidence interval has coverage $1-\alpha$. Simple!

The problem arises from approximation-- our CLT- and simulation-based CIs are based on approximations, and so their coverage will not be exactly $1-\alpha$.

Implement a function called `estimate_coverage`, whose arguments are given in the code block below, and which returns an estimate of the coverage (i.e., a number between 0 and 1).

* `CI_fn` is a *function* (in our code, this will be either `CLT_CI` or `simulation_CI`). Yes, functions can be arguments to other functions in R! Note that this argument will just be a function. *Not a string.*
* `NMC` is a positive integer, the number of replicates to run.
* `nsamp` is a positive integer, the number of samples.
* `lambdatrue` is a positive real, the parameter of the Poisson that we will use to generate our data. It should default to 1.
* `alpha` is a numeric between 0 and 1. `1-alpha` will be the (target) level of our CI.

```{r}
estimate_coverage <- function( CI_fn, NMC, nsamp, lambdatrue=1, alpha=0.05 ) {
  coverages <- 0; # Count how often the CI "catches" lambdatrue
  
  for (i in 1:NMC ) {
    # Generate data: nsamp draws from Poisson( lambdatrue )
    data <- rpois(n=nsamp, lambda=lambdatrue);
    
    # Construct a confidence interval from it using the given CI function
    CI = CI_fn(data, alpha = alpha)
    # if lambdatrue is in the CI, increment coverages.
    if (contained(CI, lambdatrue) == TRUE) {
      coverages = coverages + 1
    }
  }
  
  return( coverages/NMC )
}

```

### Part f: exploring

Wow, that was a lot of coding! Here comes the payoff!

Use the code above to estimate the coverage of your two CI methods.
Try changing the different arguments-- $\lambda$, $\alpha$, `nsamp`, etc.
Coverages of the two methods *should* both be close to $1-\alpha$, whatever you chose $\alpha$ to be, but you will likely find that they tend to follow patterns, consistently either over- or under-shooting the target.

Take some time to explore these patterns. What do you see?
There are no right or wrong answers, here.
We are just asking you to see how these two different CI methods behave in different situations.

An important point that we alluded to above is that in the Poisson, the mean and variance are both just $\lambda$.
This is actually part of why you may have noticed some weird behavior in your experiments above.
Unfortunately, a thorough explanation of that weird behavior will have to wait for your later theory courses.

```{r}
estimate_coverage(CI_fn = simulation_CI, NMC = 1000, nsamp = 30, lambdatrue = 1, alpha = .05)

estimate_coverage(CI_fn = CLT_CI, NMC = 1000, nsamp = 30, lambdatrue = 1, alpha = .05)

estimate_coverage(CI_fn = simulation_CI, NMC = 1000, nsamp = 30, lambdatrue = 1, alpha = .1)

estimate_coverage(CI_fn = CLT_CI, NMC = 1000, nsamp = 30, lambdatrue = 1, alpha = .1)
```

***

The CLT confidence intervals are closer to the true confidence rate of (1 - alpha) than the Simulation confidence intervals. Additionally, the CLT confidence interval consistently underestimates while the simulation confidence intervals consistently overestimates.

***

## 2) The infamous mule kick data

The file `mule_kicks.csv`, available for download (here)[https://kdlevin-uwstat.github.io/STAT340-Fall2021/hw/03/mule_kicks.csv], contains a simplified version of a very famous data set.
The data consists of the number of soldiers killed by being kicked by mules or horses each year in a number of different companies in the Prussian army near the end of the 19th century.

This may seem at first to be a very silly thing to collect data about, but it is a very interesting thing to look at if you are interested in rare events.
Deaths by horse kick were rare events that occurred independently of one another, and thus it is precisely the kind of process that we might expect to obey a Poisson distribution.

Download the data and read it into R by running
```{r}
download.file('https://kdlevin-uwstat.github.io/STAT340-Fall2021/hw/03/mule_kicks.csv', destfile='mule_kicks.csv');
mule_kicks <- read.csv('mule_kicks.csv', header=TRUE);

head(mule_kicks);
```

`mule_kicks` contains a single column, called `deaths`.
Each entry is the number of soldiers killed in one corps of the Prussian army in one year.
There are 14 corps in the data set, studied over 20 years, for a total of 280 death counts.

### Part a: estimating the Poisson rate

Assuming that the mule kicks data follows a Poisson distribution, produce a point estimate for the rate parameter $\lambda$.
There are no strictly right or wrong answers, here, though there are certainly better or worse ones.

```{r}
#TODO: estimate the rate parameter.

lambdahat <- mean(mule_kicks$deaths); # TODO: write code; store your estimate in lambdahat.
```

### Part b: constructing a CI

Using everything you know (Monte Carlo, CLT, etc.), construct a confidence interval for the rate parameter $\lambda$.
Explain in reasonable detail what you are doing and why you are constructing the confidence interval in this way (a few sentences is fine!).

```{r}
alpha = .05

x_bar = mean(mule_kicks$deaths)
var_x = var(mule_kicks$deaths)
n = length(mule_kicks$deaths)

margin_of_error = abs( qnorm(alpha/2)*sqrt(var_x/n) )
  
CI = c(x_bar - margin_of_error, x_bar + margin_of_error)
CI
```

***

Given the results from question 1 which showed that the CLT confidence intervals tended have the true parameter within them at rates closer to (1 - alpha) than the simulation confidence intervals and therefore were more accurate, I decided to construct a 95% CLT confidence interval rather than a simulation confidence interval. This came out to be [0.5977054, 0.8022946].

***

### Part c: assessing a model

Here's a slightly more open-ended question.
We *assumed* that the data followed a Poisson distribution.
This may or may not be a reasonable assumption.
Use any and all tools that you know to assess (either with code or simply in words) how reasonable or unreasonable this assumption is.

Once again, there are no strictly right or wrong answers here.
Explain and defend your decisions and thought processes in a reasonable way and you will receive full credit.

***

```{r}
hist(mule_kicks$deaths)
```
The plot of the frequency of the different number of deaths in a given year plausibly appears like it can be poisson with a lambda of <= 1. 

One assumption of a poisson distribution are that individual events are independent of each other. This doesn't seem like a great assumption for the deaths by mule kicks per year at different companies for two reasons. First, individual companies are likely to have similar rates of mule kick deaths across different years compared to across different companies, thereby making different data points dependent. Second, during years where there are extra battles or wars there likely will be an increase in mule kick deaths across all companies compared to more peaceful years.

Poisson distributions also tend to model the number of arrivals per unit of time and the number of mule kick deaths per year doesn't exactly fit into this category.

However, despite the imperfect map over and the independence assumption not being perfect, the independence assumption rarely is perfectly satisfied anyways and from the plot it seems like a poisson distribution would still do a great job of modeling this distribution.

***

